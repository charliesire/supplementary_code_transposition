{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0d59c97-d40f-4181-b878-72a4c03e1eb6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Here is the description of the functions\n",
    "\n",
    "The computer code takes q variables $(\\lambda_1, \\lambda_2, \\dots, \\lambda_q)$. $p$ of them are physical variables to calibrate and are teated with uniform prior, and $q-p$ are numerical variables to represent the model error and are treated with hierarchical model.\n",
    "The array \"index_lambda_p\" indicates which of these variables are the physical variables and \"index_lambda_q\". \n",
    "\n",
    "- transform_Lambda takes a normalized vector of parameters $\\boldsymbol{\\lambda}_{norm}$, with the first $p$ variables treated as physical variable, and the next $q-p$ as the numerical variables, and return the vector $\\boldsymbol{\\lambda}$, with physical values and with each variable at the right position relatively to \"index_lambda_p\" and \"index_lambda_q\". \n",
    "\n",
    "- p_lambda_df takes a dataframe df_Lambda of vectors $(\\boldsymbol{\\lambda}_k)_{k=1}^M$ and a vector of hyperparameters $\\boldsymbol{\\alpha}$, and return the vector $p_{\\boldsymbol{\\Lambda}}(\\boldsymbol{\\lambda}_k \\mid \\boldsymbol{\\alpha})_{k=1}^M$\n",
    "\n",
    "- get_likelihoods_dflambda takes a dataframe df_Lambda of vectors $(\\boldsymbol{\\lambda}_k)_{k=1}^M$, and return the simulations, the standard deviations associated if gaussian processes are used, and the likelihoods associated to each $\\boldsymbol{\\lambda}_k.$ The argument sigma refers to the observations standard deviation; myCODE is the simulator (or the GP predictor), results_measures is the dataframe of the observations; index indicates the output considered for the computation of the likelihood (if multiple outputs are indicated, one likelihood will be computed for each output and each $\\boldsymbol{\\lambda}_k$); std_code indicates whether or not the standard deviation of the code should be considered, and idx_loo is the index of the observation that should be removed for the likelihood computation in the LOO scheme.\n",
    "\n",
    "- sample_Lambda takes an integer $M$ and a vector of hyperparemters $\\boldsymbol{\\alpha}$ and returns a sample $(\\boldsymbol{\\lambda}_k)_{k=1}^M$ i.i.d. with density $p_{\\boldsymbol{\\Lambda}}(.\\mid \\boldsymbol{\\alpha})$.\n",
    "\n",
    "- likelihood_alpha takes a vector $\\boldsymbol{\\alpha}$, the likelihoods computed with another vector $\\boldsymbol{\\alpha}^\\star$, $p_{\\boldsymbol{\\Lambda}}(\\boldsymbol{\\lambda}_k\\mid \\boldsymbol{\\alpha}^\\star)_{k=1}^{n}$ the prior densities computed with $\\boldsymbol{\\alpha}^\\star$, and the dataframe $(\\boldsymbol{\\lambda}_k)_{k=1}^M$ sampled with $p_{\\boldsymbol{\\Lambda}}(.\\mid \\boldsymbol{\\alpha}^\\star)$. It returns the estimated likelihood of $\\boldsymbol{alpha}$ with importance sampling.\n",
    "\n",
    "- find_best takes $\\boldsymbol{\\alpha}^\\star$, the likelihoods computed with $\\boldsymbol{\\alpha}^\\star$, the sample $(\\boldsymbol{\\lambda}_k)_{k=1}^M$ i.i.d with density $p_{\\boldsymbol{\\Lambda}}(.\\mid \\boldsymbol{\\alpha}^\\star)$, and returns the estimated maximum a posteriori, considering uniform prior.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e5d933-b39f-40d4-af89-d46ff955ef75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transform_Lambda(Lambda, index_lambda_p, index_lambda_q):\n",
    "    Lambda_new = [Lambda[(index_lambda_p + index_lambda_q).index(x)] for x in range(len(index_lambda_p + index_lambda_q))] #reorder vector\n",
    "    Lambda_new = np.array([Lambda_new[x]*(bMAXlambda[x] - bMINlambda[x])+bMINlambda[x] for x in range(len(Lambda_new))]) #back to physical values\n",
    "    return(Lambda_new)\n",
    "\n",
    "def p_lambda_df(df_Lambda, alpha, index_lambda_p, index_lambda_q, scale = 0.45):\n",
    "    scale = np.array([scale]*len(alpha)) #same scale for each variable\n",
    "    lambd_norm = (df_Lambda-bMINlambda)/(bMAXlambda - bMINlambda) #normalize values\n",
    "    def fun1(x): return all(0 <= coord <= 1 for coord in x) \n",
    "    coeff1 = lambd_norm.iloc[:,index_lambda_p].apply(fun1, axis=1) #Uniform [0,1] for the coordinates \"index_lambda_p\"\n",
    "    a, b = (0 - alpha) / scale, (1 - alpha) / scale \n",
    "    coeff2=1\n",
    "    for ii in range(len(alpha)):\n",
    "        coeff2 = coeff2*truncnorm.pdf((lambd_norm.iloc[:,index_lambda_q[ii]].values - alpha[ii])/scale[ii], a[ii],b[ii])/scale[ii] #truncated gaussian for the coordinates \"index_lambda_q\"\n",
    "    return coeff1*coeff2\n",
    "\n",
    "def get_likelihoods_dflambda(df_Lambda, sigma, myCODE, results_measures, index = [1,2,3], std_code = False, idx_loo = None):\n",
    "    Ysimu = myCODE(df_Lambda, index = index,  std_bool = std_code, vectorize = True, idx_loo = idx_loo) #Get simulations\n",
    "    if std_code: #if gaussian process regression\n",
    "        Ysimu, Ystd = Ysimu #Get std deviations and simulations\n",
    "        res = [[np.prod(norm.pdf(results_measures.loc[list(set(range(len(results_measures))) - set([idx_loo])),f\"Y{index[ii]}\"].values-Ysimu[iii].iloc[:,ii].values, loc=0, scale=np.sqrt(sigma[index[ii]-1] + Ystd[iii].iloc[:,ii].values))) for ii in range(len(index))] for iii in range(len(Ysimu))] #compute gaussian likelihoods, considering std of observation noise and std of gaussian process\n",
    "        return Ysimu, Ystd, np.array(res)\n",
    "    else: #if deterministic simulator\n",
    "        res = [[np.prod(norm.pdf(results_measures.loc[list(set(range(len(results_measures))) - set([idx_loo])),f\"Y{index[ii]}\"]-Ysimu[iii].iloc[:,ii], loc=0, scale=sigma[index[ii]-1])) for ii in range(len(index))] for iii in range(len(Ysimu))] #compute gaussian likelihoods, considering only observation noise\n",
    "        return Ysimu, np.array(res)\n",
    "\n",
    "def sample_Lambda(alpha, M, index_lambda_p, index_lambda_q, scale = 0.45):\n",
    "    Lambda_list = []\n",
    "    if len(index_lambda_q) > 0:\n",
    "        scale = np.array([scale]*len(alpha))\n",
    "        a, b = (0 - alpha) / scale, (1 - alpha) / scale\n",
    "    for k in range(M):\n",
    "        if len(index_lambda_q) > 0:\n",
    "            sample_lambda_q = np.array([truncnorm.rvs(a[ii], b[ii], size=1)[0]*scale[ii] + alpha[ii] for ii in range(len(alpha))]) #truncated gaussian sample\n",
    "            Lambda = transform_Lambda(np.concatenate([np.random.uniform(0,1,len(index_lambda_p)), sample_lambda_q]), index_lambda_p = index_lambda_p, index_lambda_q = index_lambda_q) #concatenate uniform sample with truncated gaussian sample, and use transform_Lambda to reorder and get physical values\n",
    "        else: \n",
    "            Lambda = transform_Lambda(np.random.uniform(0,1,len(index_lambda_p)), index_lambda_p = index_lambda_p, index_lambda_q = index_lambda_q) #only uniform sample\n",
    "        Lambda_list.append(Lambda)\n",
    "    return pd.DataFrame(np.array(Lambda_list))\n",
    "\n",
    "    \n",
    "def likelihood_alpha(alpha, likelihoods_alpha_star, denom_is, df_Lambda, index_lambda_p, index_lambda_q, scale):\n",
    "    ratio_is = np.array(p_lambda_df(df_Lambda = df_Lambda, alpha = alpha, index_lambda_p = index_lambda_p, index_lambda_q = index_lambda_q, scale = scale)/denom_is) #compute importance sampling ratios\n",
    "    ratio_is = ratio_is.reshape(len(ratio_is),1)\n",
    "    return np.mean(likelihoods_alpha_star*ratio_is) #Mean of the likelihoods weighted by the importance sampling ratios\n",
    "\n",
    "\n",
    "def find_map(alpha_star, bounds, likelihoods_alpha_star, df_Lambda, index_lambda_p, index_lambda_q, scale):\n",
    "    denom_is = p_lambda_df(df_Lambda = df_Lambda, alpha = alpha_star, index_lambda_p = index_lambda_p, index_lambda_q = index_lambda_q, scale = scale) #Compute the denomination of importance sampling ratio\n",
    "    fun = lambda alpha: likelihood_alpha(alpha = alpha, likelihoods_alpha_star = likelihoods_alpha_star, denom_is = denom_is, df_Lambda = df_Lambda, index_lambda_p = index_lambda_p, index_lambda_q = index_lambda_q, scale = scale)\n",
    "    baseline = fun(alpha_star)\n",
    "    fun_opt = lambda alpha: -fun(alpha)/baseline #Normalized by baseline so that values of functions are not too low\n",
    "    return minimize(fun_opt, alpha_star, method='L-BFGS-B', bounds=bounds).x #minimize the opposite with L-BFGS-B\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
